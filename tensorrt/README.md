## **1. ì‚¬ì „ ìš”êµ¬ ì‚¬í•­**
### **ìš´ì˜ ì²´ì œ**
- TensorRTëŠ” ì£¼ë¡œ **Ubuntu** ë° **Windows**ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. Ubuntu 18.04 ë˜ëŠ” 20.04ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.

### **GPU ë“œë¼ì´ë²„**
- NVIDIA GPUê°€ í•„ìš”í•˜ë©°, ìµœì‹  GPU ë“œë¼ì´ë²„ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
- GPU ë“œë¼ì´ë²„ ë²„ì „ í™•ì¸:
  ```bash
  nvidia-smi
  ```
- [NVIDIA ë“œë¼ì´ë²„ ë‹¤ìš´ë¡œë“œ](https://www.nvidia.com/Download/index.aspx)

### **CUDA ë° cuDNN**
- TensorRTëŠ” CUDAì™€ cuDNNì´ í•„ìš”í•©ë‹ˆë‹¤. CUDAì™€ cuDNNì„ ë¨¼ì € ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
- [CUDA Toolkit ë‹¤ìš´ë¡œë“œ](https://developer.nvidia.com/cuda-downloads)
- [cuDNN ë‹¤ìš´ë¡œë“œ](https://developer.nvidia.com/cudnn)

---

## **2. TensorRT ì„¤ì¹˜**
### **ë°©ë²• 1: NVIDIA Developer Zoneì—ì„œ ë‹¤ìš´ë¡œë“œ**
1. TensorRTë¥¼ [NVIDIA Developer Zone](https://developer.nvidia.com/tensorrt)ì—ì„œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
   - CUDAì™€ í˜¸í™˜ë˜ëŠ” TensorRT ë²„ì „ì„ ì„ íƒí•˜ì„¸ìš”.
   - `.deb` ë˜ëŠ” `.tar` íŒŒì¼ í˜•ì‹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

2. **Ubuntu**ì—ì„œ `.deb` íŒŒì¼ ì„¤ì¹˜:
   ```bash
   sudo dpkg -i nv-tensorrt-repo-<version>-cuda-<cuda-version>.deb
   sudo apt-key add /var/nv-tensorrt-repo-<version>/7fa2af80.pub
   sudo apt-get update
   sudo apt-get install tensorrt
   ```
   ì¶”ê°€ì ìœ¼ë¡œ Python ë°”ì¸ë”© ì„¤ì¹˜:
   ```bash
   sudo apt-get install python3-libnvinfer-dev
   ```

3. **Windows**ì—ì„œ ì„¤ì¹˜:
   - ì„¤ì¹˜ íŒŒì¼ì„ ì‹¤í–‰í•˜ì—¬ ì§€ì‹œì— ë”°ë¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
   - ì„¤ì¹˜ í›„ TensorRT ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œë¥¼ `PATH` í™˜ê²½ ë³€ìˆ˜ì— ì¶”ê°€í•˜ì„¸ìš”.

### **ë°©ë²• 2: Python íŒ¨í‚¤ì§€ë¡œ ì„¤ì¹˜**
TensorRTì˜ ì¼ë¶€ Python APIëŠ” PyPIë¥¼ í†µí•´ ì œê³µë©ë‹ˆë‹¤.
```bash
pip install nvidia-pyindex
pip install nvidia-tensorrt
```

---

## **3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**
TensorRT ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ì‹¤í–‰ íŒŒì¼ ê²½ë¡œë¥¼ í™˜ê²½ ë³€ìˆ˜ì— ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

### **Ubuntu**
`~/.bashrc` íŒŒì¼ì— ë‹¤ìŒì„ ì¶”ê°€:
```bash
export PATH=/usr/local/TensorRT/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/TensorRT/lib:$LD_LIBRARY_PATH
```
ë³€ê²½ ì‚¬í•­ ì ìš©:
```bash
source ~/.bashrc
```

### **Windows**
- TensorRT ì„¤ì¹˜ ê²½ë¡œë¥¼ `PATH`ì— ì¶”ê°€:
  ```
  C:\Program Files\NVIDIA GPU Computing Toolkit\TensorRT-<version>\lib
  ```

---

## **4. TensorRT Python API í…ŒìŠ¤íŠ¸**
TensorRT Python APIê°€ ì œëŒ€ë¡œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•˜ë ¤ë©´ ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•´ ë³´ì„¸ìš”:
```python
import tensorrt as trt

print(f"TensorRT Version: {trt.__version__}")
```

---

## **5. TensorRT ëª¨ë¸ ë³€í™˜ ë° ì‹¤í–‰**
### **ONNX ëª¨ë¸ì„ TensorRT ì—”ì§„ìœ¼ë¡œ ë³€í™˜**
1. TensorRTì˜ `trtexec`ë¥¼ ì‚¬ìš©í•˜ì—¬ ONNX ëª¨ë¸ì„ ë³€í™˜:
   ```bash
   trtexec --onnx=model.onnx --saveEngine=model.trt --fp16
   ```
   ì£¼ìš” ì˜µì…˜:
   - `--onnx`: ì…ë ¥ ONNX ëª¨ë¸ íŒŒì¼ ê²½ë¡œ.
   - `--saveEngine`: ì €ì¥í•  TensorRT ì—”ì§„ íŒŒì¼ ì´ë¦„.
   - `--fp16`: FP16 ìµœì í™” í™œì„±í™” (ì§€ì›í•˜ëŠ” GPUì—ì„œë§Œ ê°€ëŠ¥).

2. Python APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€í™˜:
   TensorRT Python APIë¥¼ í™œìš©í•˜ì—¬ ONNX ëª¨ë¸ì„ TensorRT ì—”ì§„ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
   ```python
   import tensorrt as trt

   TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
   builder = trt.Builder(TRT_LOGGER)
   network = builder.create_network(
       1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
   )
   parser = trt.OnnxParser(network, TRT_LOGGER)

   with open("model.onnx", "rb") as model:
       if not parser.parse(model.read()):
           for error in range(parser.num_errors):
               print(parser.get_error(error))

   config = builder.create_builder_config()
   config.set_flag(trt.BuilderFlag.FP16)
   config.max_workspace_size = 1 << 30  # 1GB

   engine = builder.build_engine(network, config)
   with open("model.trt", "wb") as f:
       f.write(engine.serialize())
   ```

---

## **6. TensorRT ì—”ì§„ ì‹¤í–‰**
TensorRT ì—”ì§„ì„ ì‹¤í–‰í•˜ë ¤ë©´ Python APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:
```python
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit

# TensorRT Logger
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)

# Engine ë¡œë“œ
with open("model.trt", "rb") as f, trt.Runtime(TRT_LOGGER) as runtime:
    engine = runtime.deserialize_cuda_engine(f.read())

# Context ìƒì„±
context = engine.create_execution_context()

# ì…ë ¥/ì¶œë ¥ ë²„í¼ í• ë‹¹ ë° ì‹¤í–‰
# (ì…ë ¥ ë°ì´í„°ì™€ ì¶œë ¥ ë°ì´í„° ê´€ë¦¬ ì½”ë“œ ì¶”ê°€)
```

---

## **7. ë””ë²„ê¹… ë° ìµœì í™”**
- **`trtexec` í”„ë¡œíŒŒì¼ë§**: ëª¨ë¸ ìµœì í™” ë° ë””ë²„ê¹…ì— ìœ ìš©í•©ë‹ˆë‹¤.
  ```bash
  trtexec --onnx=model.onnx --fp16 --dumpProfile --verbose
  ```
- **ë©”ëª¨ë¦¬ ìµœì í™”**: TensorRTëŠ” ìµœëŒ€ ë©”ëª¨ë¦¬ í¬ê¸°ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì ì ˆí•œ í¬ê¸°ë¡œ ì¡°ì •í•˜ì„¸ìš”:
  ```python
  config.max_workspace_size = 1 << 30  # 1GB
  ```

---

TensorRT ì„¤ì •ì€ ì²˜ìŒì—ëŠ” ë³µì¡í•´ ë³´ì´ì§€ë§Œ, ì„¤ì •ì´ ì™„ë£Œë˜ë©´ ë§¤ìš° ë¹ ë¥¸ ì¶”ë¡  ì†ë„ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬¸ì œë‚˜ ì˜ë¬¸ ì‚¬í•­ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ì§ˆë¬¸í•˜ì„¸ìš”! ğŸ˜Š
